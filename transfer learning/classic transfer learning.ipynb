{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this Python notebook, we will be employing transfer learning techniques to train existing models (AlexNet, VGG, ResNet and DenseNet). The process will be divided into two main parts:\n",
    "\n",
    "Feature Extraction: In the first part, we will utilize the models as fixed feature extractors. This involves modifying only the last fully connected (FC) layer of each model to suit our specific task and training just this layer while keeping the rest of the model unchanged.\n",
    "\n",
    "Fine-Tuning: In the second part, we will implement fine-tuning, which involves training all the layers of the model on our dataset, starting from the pretrained weights. We will specifically apply this approach to AlexNet."
   ],
   "metadata": {
    "collapsed": false,
    "id": "p9zeDZ-saTFH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models, transforms\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import os\n"
   ],
   "metadata": {
    "id": "8ohK6OWX87Ej",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:36.678921Z",
     "start_time": "2024-03-28T15:38:34.599889Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "set up for data sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r_JaO-xsyhJN",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:36.684363Z",
     "start_time": "2024-03-28T15:38:36.680059Z"
    }
   },
   "outputs": [],
   "source": [
    "# create train dataloader and valid dataloader\n",
    "def add_gaussian_noise_tensor(tensor, mean=0, std=0.02):\n",
    "    noise = torch.randn_like(tensor) * std + mean\n",
    "    noisy_tensor = tensor + noise\n",
    "    return noisy_tensor\n",
    "\n",
    "def set_transform(add_noise=False):\n",
    "  train_transform = transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "  valid_transform = transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "  \n",
    "  if add_noise:\n",
    "    # Define the mean and standard deviation for Gaussian noise\n",
    "    mean_value = 0  # Mean of the Gaussian noise\n",
    "    std_value = 0.05  # Standard deviation of the Gaussian noise\n",
    "    # Define the transform function using transforms.Lambda\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: add_gaussian_noise_tensor(x, mean=mean_value, std=std_value)),  # Add Gaussian noise to \n",
    "    ])\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "      noisy_transform,\n",
    "    ])\n",
    "\n",
    "    valid_transform = transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "      noisy_transform,\n",
    "    ])\n",
    "  return train_transform, valid_transform\n",
    "\n",
    "def create_dataloader(root_dir, batch_size, add_noise=False):\n",
    "  train_transform, valid_transform = set_transform(add_noise)\n",
    "  # Create datasets\n",
    "  train_dataset = ImageFolder(root=root_dir + '/train', transform=train_transform)\n",
    "  valid_dataset = ImageFolder(root=root_dir + '/val', transform=valid_transform)\n",
    "  if add_noise:\n",
    "      num_workers = 0\n",
    "  else:\n",
    "      num_workers = 2\n",
    "  print(num_workers)\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "  valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "  dataloaders = {'train' : train_loader, 'val' : valid_loader}\n",
    "  return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting=False):\n",
    "    \"\"\"\n",
    "    Sets the .requires_grad attribute of the parameters in the model. This is used to freeze or unfreeze\n",
    "    the model's parameters during training.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model whose parameters will be modified.\n",
    "    - feature_extracting (bool): If True, the model is being used for feature extraction, and all\n",
    "      parameters are frozen to prevent gradients from being computed. If False, the model will be fine-tuned,\n",
    "      and gradients will be computed for all parameters.\n",
    "\n",
    "    If feature_extracting is True, this function goes through all parameters in the model and sets\n",
    "    .requires_grad to False, effectively freezing the model during training. This is useful when you're only\n",
    "    interested in training a few top layers of a model while keeping the rest unchanged.\n",
    "\n",
    "    Conversely, if feature_extracting is False, this indicates that the model will undergo fine-tuning,\n",
    "    where all model parameters are subject to training. Therefore, it sets .requires_grad to True for all\n",
    "    parameters, allowing gradients to be computed and weights to be updated during training.\n",
    "    \"\"\"\n",
    "    if feature_extracting:\n",
    "        # Freeze all parameters for feature extraction\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        # Unfreeze all parameters for fine-tuning\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n"
   ],
   "metadata": {
    "id": "B3yEnofB0rPN",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:36.687141Z",
     "start_time": "2024-03-28T15:38:36.684989Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, set_lora=False):\n",
    "    \"\"\"\n",
    "    Initializes a model for transfer learning based on the specified architecture (model_name).\n",
    "    This function can adjust models for a new task by changing their output layers to match\n",
    "    the number of classes for the new task (num_classes). It supports operating in either\n",
    "    feature extraction or fine-tuning mode.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the model architecture to be initialized.\n",
    "    - num_classes (int): The number of classes for the new task.\n",
    "    - feature_extract (bool): If True, the model is set to feature extraction mode where only the final layer's parameters are updated.\n",
    "    - use_pretrained (bool): If True, initializes the model with weights pre-trained on ImageNet.\n",
    "    - set_lora (bool): Placeholder for future use, not implemented in this function.\n",
    "\n",
    "    Returns:\n",
    "    - model_ft (torch.nn.Module): The initialized model ready for training on the new task.\n",
    "    - input_size (int): The required input image size for the model, e.g., 224 for AlexNet.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables for the model and input size.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    # Define weights setting based on whether pretrained weights are desired.\n",
    "    weights = 'DEFAULT' if use_pretrained else None\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        # Initialize ResNet\n",
    "        model_ft = models.resnet18(weights=weights)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        # Initialize AlexNet\n",
    "        model_ft = models.alexnet(weights=weights)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        # Initialize VGG\n",
    "        model_ft = models.vgg16(weights=weights)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        # Initialize DenseNet\n",
    "        model_ft = models.densenet121(weights=weights)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        # Note: DenseNet's input size is often larger in practice, but 224 works for compatibility.\n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"The specified model name is not supported.\")\n",
    "\n",
    "    return model_ft, input_size"
   ],
   "metadata": {
    "id": "xS3JI_lW0CJg",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:37.598036Z",
     "start_time": "2024-03-28T15:38:37.591349Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def params_to_update(model, feature_extract):\n",
    "    \"\"\"\n",
    "    Identifies and returns the parameters to be optimized/updated in this run. If we are\n",
    "    fine-tuning, we will update all parameters. However, if we are using the feature extraction\n",
    "    method, we will only update the parameters that were recently initialized, i.e., the parameters\n",
    "    for which requires_grad is True.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model for which parameters are to be identified.\n",
    "    - feature_extract (bool): Flag indicating whether we are feature extracting. If True, only parameters\n",
    "      that require gradients are updated; otherwise, all parameters are updated.\n",
    "\n",
    "    Returns:\n",
    "    - params_to_update (list of torch.nn.parameter.Parameter): A list of parameters that will be updated.\n",
    "    \"\"\"\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        print(\"Params to learn:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        # In the case of fine-tuning, we consider all parameters for updates,\n",
    "        # but still filter them based on requires_grad to be consistent with the condition.\n",
    "        # This allows the function to be flexible for models where some parameters might explicitly be frozen.\n",
    "        params_to_update = [param for param in model.parameters() if param.requires_grad]\n",
    "        print(\"Params to learn:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    return params_to_update\n"
   ],
   "metadata": {
    "id": "Ich73pbl37Jn",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:38.450372Z",
     "start_time": "2024-03-28T15:38:38.446340Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, model_name, dataloaders, criterion, optimizer, num_epochs=25, set_lora=False):\n",
    "    \"\"\"\n",
    "    Train and validate a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be trained and validated.\n",
    "    - model_name: Name of the model, used for saving checkpoints.\n",
    "    - dataloaders: A dictionary containing 'train' and 'val' DataLoaders for loading the data.\n",
    "    - criterion: The loss function.\n",
    "    - optimizer: The optimization algorithm.\n",
    "    - num_epochs (int): The number of epochs to train the model.\n",
    "    - set_lora (bool): Flag for potential future use, not used in the current implementation.\n",
    "\n",
    "    Returns:\n",
    "    - model: The trained model.\n",
    "    - results: A df with validation accuracy, validation loss, rtain accuracy and train loss for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    # Store the original model parameters to restore later if needed.\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    results_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch goes through both training and validation phases\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data in batches.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                # Track history if in train phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Backward pass and optimize if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            epoch_acc = epoch_acc.cpu().numpy()  # Convert tensor to CPU\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            results_list.append(pd.DataFrame({'phase': [phase], 'epoch': [epoch], 'loss': [epoch_loss], 'accuracy': [epoch_acc.item()]}))\n",
    "\n",
    "            # Save the model if it has the best validation accuracy so far\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Print out the training time and best validation accuracy\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    results = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "    # Load best model weights and save the model checkpoint\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    if not os.path.isdir('checkpoints'):\n",
    "        os.mkdir('checkpoints')\n",
    "    torch.save({'net': model.state_dict()}, f'./checkpoints/{model_name}.pth')\n",
    "\n",
    "    return model, results\n"
   ],
   "metadata": {
    "id": "_hAAmM_Q17o9",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:39.269501Z",
     "start_time": "2024-03-28T15:38:39.261419Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a PyTorch model on a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to evaluate.\n",
    "    - dataloader (torch.utils.data.DataLoader): DataLoader for the dataset to evaluate the model on.\n",
    "    - device (torch.device): The device on which to perform the calculations (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model_accuracy (float): The accuracy of the model on the dataset as a percentage.\n",
    "    - confusion_matrix (np.array): A confusion matrix of size [num_classes, num_classes], where\n",
    "      num_classes is the number of unique labels in the dataset. The matrix is used to evaluate\n",
    "      the model's performance across different classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Switch the model to evaluation mode to deactivate Dropout and use learned statistics in BatchNorm.\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    # Assuming a maximum of 38 classes - adjust the size according to your specific task.\n",
    "    confusion_matrix = np.zeros([38, 38], int)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get the model's predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update total images and correctly predicted counts\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update confusion matrix\n",
    "            for i, l in enumerate(labels):\n",
    "                confusion_matrix[l.item(), predicted[i].item()] += 1\n",
    "\n",
    "    # Calculate accuracy as the percentage of correct predictions\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "\n",
    "    return model_accuracy, confusion_matrix"
   ],
   "metadata": {
    "id": "HkTj4clCD-bJ",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:40.338694Z",
     "start_time": "2024-03-28T15:38:40.333286Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "config device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using MPS\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_built():\n",
    "    print(\"using MPS\")\n",
    "    device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:41.746191Z",
     "start_time": "2024-03-28T15:38:41.744091Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "set parameters for training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def set_and_run_model(root_dir, model_name, file_name, feature_extract, add_noise=False):\n",
    "    \"\"\"\n",
    "    Set up and run the model with the specified configurations.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory of the dataset.\n",
    "        model_name (str): Name of the model to be used (e.g., 'resnet', 'alexnet', 'vgg', 'densenet').\n",
    "        file_name (str): Name of the file to save the results.\n",
    "        feature_extract (bool): Flag for feature extraction.\n",
    "                                When False, fine-tune the whole model.\n",
    "                                When True, update only the reshaped layer parameters.\n",
    "\n",
    "    Returns:\n",
    "        model_ft: The trained model.\n",
    "    \"\"\"\n",
    "    root_dir = root_dir\n",
    "    batch_size = 128\n",
    "    dataloaders = create_dataloader(root_dir, batch_size, add_noise)\n",
    "    # Models to choose from [resnet, alexnet, vgg, densenet]\n",
    "    model_name = model_name\n",
    "    # Number of classes in the dataset\n",
    "    num_classes = 38\n",
    "    # Batch size for training (change depending on how much memory you have)\n",
    "    batch_size = 32\n",
    "    # Number of epochs to train for\n",
    "    num_epochs = 15\n",
    "    # Flag for feature extracting. When False, we fine-tune the whole model,\n",
    "    # when True we only update the reshaped layer params\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "    # Print the model we just instantiated\n",
    "    print(model_ft)\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    updated_params = params_to_update(model_ft, feature_extract)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.backends.mps.is_built():\n",
    "        print(\"using MPS\")\n",
    "        device = torch.device(\"mps\")\n",
    "    model_ft = model_ft.to(device)\n",
    "    optimizer_ft = torch.optim.Adam(updated_params, lr=0.0002)\n",
    "    # Setup the loss fn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Train and evaluate\n",
    "    model_ft, results = train_model(model_ft, model_name, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)\n",
    "    results.to_csv(f\"results_{file_name}.csv\", index=False)\n",
    "    return model_ft"
   ],
   "metadata": {
    "id": "-EaMBxMzaTFP",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:40:04.265267Z",
     "start_time": "2024-03-28T15:40:04.259774Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_test_data(root_dir, model, std_noise=None):\n",
    "    \"\"\"\n",
    "    Run the test data on the trained model and calculate the test accuracy.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory of the dataset.\n",
    "        model: The trained model.\n",
    "        batch_size (int): Batch size for loading the test data.\n",
    "\n",
    "    Returns:\n",
    "        float: Test accuracy.\n",
    "    \"\"\"    \n",
    "    if std_noise:\n",
    "        mean_value = 0  # Mean of the Gaussian noise\n",
    "        std_value = std_noise # Standard deviation of the Gaussian noise\n",
    "    \n",
    "        # Define the transform function using transforms.Lambda\n",
    "        noisy_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: add_gaussian_noise_tensor(x, mean=mean_value, std=std_value)),  # Add Gaussian noise to \n",
    "        ])\n",
    "        test_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        noisy_transform,\n",
    "        ])\n",
    "        num_workers = 0\n",
    "    else: \n",
    "        test_transform = transforms.Compose([\n",
    "          transforms.RandomResizedCrop(224),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "\n",
    "      ])\n",
    "        num_workers = 2\n",
    "    \n",
    "    batch_size = 128    \n",
    "    test_dataset = ImageFolder(root=root_dir + '/test', transform=test_transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "    test_accuracy, _ = calculate_accuracy(model, test_loader, device)\n",
    "    print(f\"test accuracy:{test_accuracy}\")"
   ],
   "metadata": {
    "id": "1wapOdVlaTFP",
    "ExecuteTime": {
     "end_time": "2024-03-28T17:40:22.163816Z",
     "start_time": "2024-03-28T17:40:22.157438Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "root_dir = r'/Users/noambuzaglo/Desktop/lilush/PlantVillage'"
   ],
   "metadata": {
    "id": "jtvpdiU2aTFP",
    "ExecuteTime": {
     "end_time": "2024-03-28T15:38:46.603769Z",
     "start_time": "2024-03-28T15:38:46.601664Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=38, bias=True)\n",
      "  )\n",
      ")\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.2427 Acc: 0.6566\n",
      "val Loss: 0.6711 Acc: 0.7969\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.1103 Acc: 0.7071\n",
      "val Loss: 0.7150 Acc: 0.7970\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.0911 Acc: 0.7247\n",
      "val Loss: 0.5763 Acc: 0.8252\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.0823 Acc: 0.7298\n",
      "val Loss: 0.6467 Acc: 0.8125\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.0800 Acc: 0.7340\n",
      "val Loss: 0.5968 Acc: 0.8283\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.0803 Acc: 0.7378\n",
      "val Loss: 0.5773 Acc: 0.8310\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.0628 Acc: 0.7395\n",
      "val Loss: 0.5770 Acc: 0.8340\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.0777 Acc: 0.7396\n",
      "val Loss: 0.6015 Acc: 0.8304\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 1.0456 Acc: 0.7486\n",
      "val Loss: 0.5501 Acc: 0.8382\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 1.0800 Acc: 0.7445\n",
      "val Loss: 0.5569 Acc: 0.8442\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 1.0879 Acc: 0.7447\n",
      "val Loss: 0.5623 Acc: 0.8393\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 1.0640 Acc: 0.7493\n",
      "val Loss: 0.5343 Acc: 0.8493\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 1.0666 Acc: 0.7494\n",
      "val Loss: 0.6003 Acc: 0.8336\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 1.0626 Acc: 0.7535\n",
      "val Loss: 0.5437 Acc: 0.8449\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 1.0601 Acc: 0.7526\n",
      "val Loss: 0.5338 Acc: 0.8481\n",
      "\n",
      "Training complete in 230m 31s\n",
      "Best val Acc: 0.849277\n"
     ]
    }
   ],
   "source": [
    "# vgg with feature extract\n",
    "vgg_feature_extract = set_and_run_model(root_dir, 'vgg', 'vgg_feature_extract', True)"
   ],
   "metadata": {
    "id": "-75G4zG032-v",
    "ExecuteTime": {
     "end_time": "2024-03-21T10:24:40.991814Z",
     "start_time": "2024-03-21T06:34:07.998291Z"
    },
    "outputId": "0c68aefb-bcff-4025-88fd-b9541b2405fc"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "run_test_data(root_dir, vgg_feature_extract)"
   ],
   "metadata": {
    "id": "Cq2x70BtAK4y",
    "ExecuteTime": {
     "end_time": "2024-03-21T10:30:14.620390Z",
     "start_time": "2024-03-21T10:28:58.520426Z"
    },
    "outputId": "150f506e-b908-4d47-d125-bf74d44d26d2"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:84.75046210720888\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=38, bias=True)\n",
      "  )\n",
      ")\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.6758 Acc: 0.6963\n",
      "val Loss: 0.9980 Acc: 0.8095\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.5006 Acc: 0.7578\n",
      "val Loss: 0.8894 Acc: 0.8351\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.5135 Acc: 0.7674\n",
      "val Loss: 0.9347 Acc: 0.8324\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.4603 Acc: 0.7783\n",
      "val Loss: 0.9192 Acc: 0.8449\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.4604 Acc: 0.7875\n",
      "val Loss: 0.9050 Acc: 0.8458\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.4223 Acc: 0.7900\n",
      "val Loss: 1.0803 Acc: 0.8386\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.4374 Acc: 0.7904\n",
      "val Loss: 0.9248 Acc: 0.8578\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.4484 Acc: 0.7924\n",
      "val Loss: 0.9618 Acc: 0.8474\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 1.4446 Acc: 0.7971\n",
      "val Loss: 0.8594 Acc: 0.8628\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 1.4051 Acc: 0.8006\n",
      "val Loss: 0.9002 Acc: 0.8550\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 1.4269 Acc: 0.8014\n",
      "val Loss: 1.0303 Acc: 0.8483\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 1.4251 Acc: 0.8035\n",
      "val Loss: 0.8900 Acc: 0.8608\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 1.4065 Acc: 0.8067\n",
      "val Loss: 0.7230 Acc: 0.8812\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 1.4246 Acc: 0.8038\n",
      "val Loss: 0.8171 Acc: 0.8670\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 1.4154 Acc: 0.8075\n",
      "val Loss: 0.9923 Acc: 0.8531\n",
      "\n",
      "Training complete in 37m 36s\n",
      "Best val Acc: 0.881226\n"
     ]
    }
   ],
   "source": [
    "# alexnet with feature extract\n",
    "alexnet_feature_extract = set_and_run_model(root_dir, 'alexnet', 'alexnet_feature_extract', True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T11:11:47.610665Z",
     "start_time": "2024-03-21T10:34:10.596847Z"
    },
    "id": "yTOLEi-gaTFS",
    "outputId": "787df212-b9df-4935-d1f9-99a29107ddda"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:84.84288354898337\n"
     ]
    }
   ],
   "source": [
    "run_test_data(root_dir, alexnet_feature_extract)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T11:12:47.490526Z",
     "start_time": "2024-03-21T11:12:26.231603Z"
    },
    "id": "UviDxnTRaTFT",
    "outputId": "d77fc545-d4a3-418d-dfe0-73e06df83034"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=38, bias=True)\n",
      ")\n",
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.2229 Acc: 0.6635\n",
      "val Loss: 0.4935 Acc: 0.8592\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.8422 Acc: 0.7553\n",
      "val Loss: 0.4447 Acc: 0.8696\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.7802 Acc: 0.7736\n",
      "val Loss: 0.4333 Acc: 0.8672\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.7503 Acc: 0.7794\n",
      "val Loss: 0.3882 Acc: 0.8856\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.7294 Acc: 0.7869\n",
      "val Loss: 0.4343 Acc: 0.8688\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.7268 Acc: 0.7887\n",
      "val Loss: 0.3940 Acc: 0.8826\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.7108 Acc: 0.7925\n",
      "val Loss: 0.4250 Acc: 0.8734\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.7012 Acc: 0.7943\n",
      "val Loss: 0.3687 Acc: 0.8888\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.6897 Acc: 0.7983\n",
      "val Loss: 0.3870 Acc: 0.8822\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6952 Acc: 0.7965\n",
      "val Loss: 0.3871 Acc: 0.8818\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.6959 Acc: 0.7946\n",
      "val Loss: 0.4007 Acc: 0.8806\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6786 Acc: 0.7997\n",
      "val Loss: 0.3796 Acc: 0.8907\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.6771 Acc: 0.7996\n",
      "val Loss: 0.4016 Acc: 0.8786\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.6855 Acc: 0.8010\n",
      "val Loss: 0.3835 Acc: 0.8861\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6703 Acc: 0.8028\n",
      "val Loss: 0.3698 Acc: 0.8902\n",
      "\n",
      "Training complete in 196m 11s\n",
      "Best val Acc: 0.890710\n"
     ]
    }
   ],
   "source": [
    "# densenet with feature extract\n",
    "densenet_feature_extract = set_and_run_model(root_dir, 'densenet', 'densenet_feature_extract', True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T19:55:04.281291Z",
     "start_time": "2024-03-21T16:38:51.935591Z"
    },
    "id": "nWUtM9YcaTFT",
    "outputId": "3977b6dc-3e37-42a9-bf17-cab31e175dda"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:88.56284658040666\n"
     ]
    }
   ],
   "source": [
    "run_test_data(root_dir, densenet_feature_extract)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T22:07:00.900098Z",
     "start_time": "2024-03-21T22:06:08.863369Z"
    },
    "id": "jCf6tqYUaTFU",
    "outputId": "429ac519-8f43-47a0-8c63-7622610530b3"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=38, bias=True)\n",
      ")\n",
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.2890 Acc: 0.6430\n",
      "val Loss: 0.6045 Acc: 0.8249\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.9397 Acc: 0.7275\n",
      "val Loss: 0.5324 Acc: 0.8436\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.8789 Acc: 0.7455\n",
      "val Loss: 0.4939 Acc: 0.8550\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.8503 Acc: 0.7529\n",
      "val Loss: 0.4869 Acc: 0.8572\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.8351 Acc: 0.7582\n",
      "val Loss: 0.4596 Acc: 0.8608\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.8224 Acc: 0.7584\n",
      "val Loss: 0.4459 Acc: 0.8675\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.8222 Acc: 0.7612\n",
      "val Loss: 0.4616 Acc: 0.8621\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.8015 Acc: 0.7657\n",
      "val Loss: 0.4635 Acc: 0.8646\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.7919 Acc: 0.7663\n",
      "val Loss: 0.4484 Acc: 0.8672\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.7967 Acc: 0.7672\n",
      "val Loss: 0.4444 Acc: 0.8681\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.7877 Acc: 0.7706\n",
      "val Loss: 0.4174 Acc: 0.8746\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.8056 Acc: 0.7653\n",
      "val Loss: 0.4590 Acc: 0.8693\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.7975 Acc: 0.7682\n",
      "val Loss: 0.4549 Acc: 0.8653\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.7841 Acc: 0.7707\n",
      "val Loss: 0.4682 Acc: 0.8664\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.7850 Acc: 0.7710\n",
      "val Loss: 0.4914 Acc: 0.8526\n",
      "\n",
      "Training complete in 58m 40s\n",
      "Best val Acc: 0.8746\n"
     ]
    }
   ],
   "source": [
    "# resnet with feature extract\n",
    "resnet_feature_extract = set_and_run_model(root_dir, 'resnet', 'resnet_feature_extract', True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:10:43.144164Z",
     "start_time": "2024-03-24T12:12:02.841927Z"
    },
    "id": "14N1bUq7aTFU",
    "outputId": "b46ca387-ce32-40b4-80c8-71003b9b89f3"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:86.87615526802219\n"
     ]
    }
   ],
   "source": [
    "run_test_data(root_dir, resnet_feature_extract)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T14:01:36.659778Z",
     "start_time": "2024-03-24T14:01:10.126199Z"
    },
    "id": "7BAplM51aTFU",
    "outputId": "79f0b7d3-75fa-4a9c-c7af-c0b14fcf0cd2"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=38, bias=True)\n",
      "  )\n",
      ")\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.8063 Acc: 0.7692\n",
      "val Loss: 0.3476 Acc: 0.8916\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.3441 Acc: 0.8918\n",
      "val Loss: 0.2255 Acc: 0.9277\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.2698 Acc: 0.9141\n",
      "val Loss: 0.1930 Acc: 0.9373\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.9261\n",
      "val Loss: 0.1901 Acc: 0.9392\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9338\n",
      "val Loss: 0.1801 Acc: 0.9417\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.1839 Acc: 0.9402\n",
      "val Loss: 0.1362 Acc: 0.9582\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1723 Acc: 0.9426\n",
      "val Loss: 0.1515 Acc: 0.9508\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1550 Acc: 0.9488\n",
      "val Loss: 0.1383 Acc: 0.9569\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.9513\n",
      "val Loss: 0.1174 Acc: 0.9621\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9551\n",
      "val Loss: 0.1205 Acc: 0.9614\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9569\n",
      "val Loss: 0.1215 Acc: 0.9606\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.1271 Acc: 0.9583\n",
      "val Loss: 0.1141 Acc: 0.9645\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.9604\n",
      "val Loss: 0.1104 Acc: 0.9656\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.9611\n",
      "val Loss: 0.1087 Acc: 0.9655\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1127 Acc: 0.9628\n",
      "val Loss: 0.1144 Acc: 0.9616\n",
      "\n",
      "Training complete in 50m 22s\n",
      "Best val Acc: 0.965565\n"
     ]
    }
   ],
   "source": [
    "# alexnet with fine-tuning\n",
    "alexnet_fine_tuning = set_and_run_model(root_dir, 'alexnet', 'alexnet_fine-tuning', False)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T16:19:08.660329Z",
     "start_time": "2024-03-21T15:28:44.036534Z"
    },
    "id": "vyH_uUvtaTFV",
    "outputId": "6043ceaa-34f8-45a8-a35e-9a8022b18e6a"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:96.85767097966729\n"
     ]
    }
   ],
   "source": [
    "run_test_data(root_dir, alexnet_fine_tuning)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T16:19:50.189592Z",
     "start_time": "2024-03-21T16:19:27.052046Z"
    },
    "id": "na8K3TwraTFV",
    "outputId": "3511c83c-334c-405c-a754-32d32eee1cb3"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=38, bias=True)\n",
      ")\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.4020 Acc: 0.6759\n",
      "val Loss: 0.5476 Acc: 0.8715\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.4082 Acc: 0.9026\n",
      "val Loss: 0.2859 Acc: 0.9297\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.2525 Acc: 0.9370\n",
      "val Loss: 0.2066 Acc: 0.9479\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 0.9501\n",
      "val Loss: 0.1692 Acc: 0.9548\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.1641 Acc: 0.9552\n",
      "val Loss: 0.1465 Acc: 0.9591\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9618\n",
      "val Loss: 0.1252 Acc: 0.9663\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9654\n",
      "val Loss: 0.1125 Acc: 0.9675\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1146 Acc: 0.9673\n",
      "val Loss: 0.1026 Acc: 0.9710\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.9709\n",
      "val Loss: 0.0995 Acc: 0.9686\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 0.9732\n",
      "val Loss: 0.0922 Acc: 0.9731\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.0894 Acc: 0.9741\n",
      "val Loss: 0.0848 Acc: 0.9742\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9759\n",
      "val Loss: 0.0825 Acc: 0.9759\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.0826 Acc: 0.9755\n",
      "val Loss: 0.0785 Acc: 0.9769\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9778\n",
      "val Loss: 0.0768 Acc: 0.9775\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.9797\n",
      "val Loss: 0.0700 Acc: 0.9775\n",
      "\n",
      "Training complete in 135m 20s\n",
      "Best val Acc: 0.9775\n"
     ]
    }
   ],
   "source": [
    "# resnet with fine-tuning\n",
    "resnet_fine_tuning = set_and_run_model(root_dir, 'resnet', 'resnet_fine-tuning', False)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T16:16:57.538462Z",
     "start_time": "2024-03-24T14:01:36.663212Z"
    },
    "id": "bhlRrf6yaTFV",
    "outputId": "e57a8aa9-f101-4985-bffb-ce888c83d628"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:97.98983364140481\n"
     ]
    }
   ],
   "source": [
    "run_test_data(root_dir, resnet_fine_tuning)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T16:17:23.392700Z",
     "start_time": "2024-03-24T16:16:57.545237Z"
    },
    "id": "zJXZ5aPtaTFV",
    "outputId": "a3fb0599-fbd3-4d4e-bc24-6ab0a1d06210"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "add noise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=38, bias=True)\n",
      "  )\n",
      ")\n",
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.3.weight\n",
      "\t features.3.bias\n",
      "\t features.6.weight\n",
      "\t features.6.bias\n",
      "\t features.8.weight\n",
      "\t features.8.bias\n",
      "\t features.10.weight\n",
      "\t features.10.bias\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\t classifier.4.weight\n",
      "\t classifier.4.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "using MPS\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.5786 Acc: 0.8278\n",
      "val Loss: 0.2714 Acc: 0.9139\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.2667 Acc: 0.9136\n",
      "val Loss: 0.1823 Acc: 0.9417\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.2118 Acc: 0.9311\n",
      "val Loss: 0.1571 Acc: 0.9472\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9389\n",
      "val Loss: 0.1549 Acc: 0.9528\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.1685 Acc: 0.9466\n",
      "val Loss: 0.1321 Acc: 0.9587\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.1553 Acc: 0.9503\n",
      "val Loss: 0.1240 Acc: 0.9594\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9503\n",
      "val Loss: 0.1342 Acc: 0.9564\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1418 Acc: 0.9545\n",
      "val Loss: 0.1397 Acc: 0.9557\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1358 Acc: 0.9570\n",
      "val Loss: 0.1173 Acc: 0.9639\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9601\n",
      "val Loss: 0.1347 Acc: 0.9555\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9594\n",
      "val Loss: 0.1051 Acc: 0.9649\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.1250 Acc: 0.9599\n",
      "val Loss: 0.0953 Acc: 0.9705\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.9607\n",
      "val Loss: 0.1101 Acc: 0.9673\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1119 Acc: 0.9636\n",
      "val Loss: 0.1013 Acc: 0.9672\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.9648\n",
      "val Loss: 0.0975 Acc: 0.9689\n",
      "\n",
      "Training complete in 105m 57s\n",
      "Best val Acc: 0.9705\n"
     ]
    }
   ],
   "source": [
    "alexnet_fine_tuning_with_noise = set_and_run_model(root_dir, 'alexnet', 'alexnet_fine-tuning_with_noise', False, add_noise=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T17:29:27.478696Z",
     "start_time": "2024-03-28T15:43:29.145273Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std noise = 0\n",
      "test accuracy:96.69593345656192\n",
      "std noise = 0.05\n",
      "test accuracy:97.01940850277265\n",
      "std noise = 0.07\n",
      "test accuracy:97.08872458410352\n",
      "std noise = 0.09\n",
      "test accuracy:96.9731977818854\n"
     ]
    }
   ],
   "source": [
    "std_noises = [0, 0.05, 0.07, 0.09]\n",
    "for noise in std_noises:\n",
    "    print(f\"std noise = {noise}\")\n",
    "    run_test_data(root_dir, alexnet_fine_tuning_with_noise, std_noise=noise)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T17:43:52.014576Z",
     "start_time": "2024-03-28T17:42:32.104877Z"
    }
   },
   "execution_count": 21
  }
 ]
}
